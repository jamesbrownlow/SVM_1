---
title: "Example SVM"
author: "Mr. Forgetful"
date: "September 13, 2019"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SVM example
```{r}
library(e1071)
```

## load function svm

```{r eval=FALSE}
?svm
```

## load the Iris data

```{r}
data(iris)
attach(iris)
# View(iris)
```


## classification mode
# default with factor response:
```{r}
model = svm(Species ~ ., data = iris)
print(model)
summary(model)
```


# alternatively the traditional interface:
```{r}
x <- subset(iris, select = -Species)
y <- Species

trainIndex = sample(1:150, 100, replace=FALSE)
xTrain = x[trainIndex,]
yTrain = y[trainIndex]

model <- svm(xTrain, yTrain) 

print(model)
summary(model)

xTest = x[-trainIndex,]
yTest = y[-trainIndex]

# test with train data
pred <- predict(model, xTest)
# (same as:)
#pred <- fitted(model)

# Check accuracy:
table(pred, yTest)

# compute decision values and probabilities:
pred <- predict(model, xTest, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]

# visualize (classes by color, SV by crosses):
plot(cmdscale(dist(iris[,-5])),
     col = as.integer(iris[,5]),
     pch = c("o","+")[1:150 %in% model$index + 1])
```



# try regression mode on two dimensions

## create data

```{r}

x <- seq(0.1, 5, by = 0.05)
y <- log(x) + rnorm(x, sd = 0.2)

```


## estimate model and predict input values

```{r}
m   <- svm(x, y)
new <- predict(m, x)

```


## visualize

```{r}
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)
legend('topleft', c('septal width', 'log (x)', 'predicted y'), col=c(1,2,4), lty=1)

```


## density-estimation

### create 2-dim. normal with rho=0:

```{r}
X <- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)
```

### traditional approach:

```{r}
m <- svm(X, gamma = 0.1)
```

### formula interface:

```{r}
m <- svm(~ a + b, gamma = 0.1)
```

Notice than an alternative forumlation is   
m <- svm(~., data = X, gamma = 0.1)


### test:

```{r}
newdata <- data.frame(a = c(0, 4), b = c(0, 4))
predict (m, newdata)
```


### visualize:

```{r}
plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)
```


### weights: (example not particularly sensible)

```{r}
i2 <- iris
levels(i2$Species)[3] <- "versicolor"
summary(i2$Species)
wts <- 100 / table(i2$Species)
wts
m <- svm(Species ~ ., data = i2, class.weights = wts)
```

### extract coefficients for linear kernel
a) regression 

```{r}
x <- 1:100
y <- x + rnorm(100)
m <- svm(y ~ x, scale = FALSE, kernel = "linear")
coef(m)
plot(y ~ x)
abline(m, col = "red")

```

b) classification

### transform iris data to binary problem, and scale data
```{r}
setosa <- as.factor(iris$Species == "setosa")
iris2 = scale(iris[,-5])

```

### fit binary C-classification model

```{r}
modelLinear <- svm(setosa ~ Petal.Width + Petal.Length,
        data = iris2, kernel = "linear")
```

## plot data and separating hyperplane
include the margin and mark support vectors
```{r}
plot(Petal.Length ~ Petal.Width, data = iris2, col = setosa)
(cf <- coef(modelLinear))
abline(-cf[1]/cf[3], -cf[2]/cf[3], col = "red")
abline(-(cf[1] + 1)/cf[3], -cf[2]/cf[3], col = "blue")
abline(-(cf[1] - 1)/cf[3], -cf[2]/cf[3], col = "blue")
points(m$SV, pch = 5, cex = 2)
```


